{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Generacion de texto con GPT2 de KerasNLP\n\n**Author:** David Alonso Quispe Castillo<br>\n**Date created:** 14/07/2023<br>\n**Last modified:** 14/07/2023<br>\n**Objetivo:** Usar el modelo GPT-2 de KerasNLP y los generadores de texto (samplers) para la generación de texto.","metadata":{"id":"inev2z-Zzb7w"}},{"cell_type":"markdown","source":"En este proyecto, basado en el trabajo previo de [Chen Qian](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/gpt2_text_generation_with_kerasnlp.ipynb#scrollTo=76gl9HSIF-uB), utilizaremos [KerasNLP](https://keras.io/keras_nlp/) para cargar un modelo de lenguaje grande  (LLM) pre-entrenado - el modelo [GPT-2 ](https://openai.com/research/better-language-models), desarrollado por OpenAI. Aprovecharemos las capacidades de generación de texto de este modelo para crear texto basado en una entrada proporcionada por el usuario. Además, se demostrará cómo GPT-2 puede adaptarse rápidamente a otros idiomas, como el español.","metadata":{"id":"lniu_DmLzb70"}},{"cell_type":"markdown","source":"https://www.tensorflow.org/install/source?hl=es-419#gpu","metadata":{"id":"CWbCbP9BwYmY"}},{"cell_type":"markdown","source":"##  Antes de comenzar (Colab)\n\nColab ofrece diferentes tipos de entornos de ejecución. Asegúrate de ir a **Entorno de ejecución -> Cambiar tipo de entorno de ejecución** y seleccionar el entorno de ejecución con aceleración de hardware GPU (que debería tener >12 GB de RAM del host y ~15 GB de RAM de la GPU) ya que ajustaremos finamente el modelo GPT-2. Ejecutar este tutorial en un entorno de ejecución de CPU tomaría horas.\n\nen colab las versiones usadas son:\n\n\n\nVersión de keras-nlp: 0.6.0\n\nVersión de TensorFlow: 2.13.0\n\nVersión de Python: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]\n\nVersión de Cudnn:\ndefine CUDNN_MAJOR 8\ndefine CUDNN_MINOR 9\ndefine CUDNN_PATCHLEVEL 0\ndefine CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n\n/* cannot use constexpr here since this is a C-only file */\n\nVersión de CUDA :\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\n\nDispositivo GPU : /physical_device:GPU:0\n\nEscpecificaciones de gpu:\nWed Jul 19 21:24:51 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P8     9W /  70W |      3MiB / 15360MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","metadata":{"id":"VWjhy_btzb71"}},{"cell_type":"markdown","source":"##  Antes de comenzar (en Amazon Sagemaker Studio lab)\n\nSe optó por usar estas versiones para la ejecución:\n- Python: 3.9.15\n- Tensorflow: 2.11.0\n- keras-nlp: 0.6.0\n\nLa razon de usar dichas versiones es porque sage maker studio lab, viene por defecto con:\n- CUDA: 11.2\n- cudnn: 8.1\n\n\nSi solamente instalas tensorflow en tu conda enviroment, y quieres ejecutar la funcion generate() del modelo te aparecerá un error de dependecia por parte del paquete tensorflow-text, y si intentas realizar la instalacion de la version correcta mediante el siguiente codigo: pip install tensorflow-text~=2.11.0, no se instalará esa version, sino la mas reciente que es la 2.13.0 y ademas que instalará toda la version completa de tensorflow 2.13.0, para instalar la version necesaria que es la 2.11.0 debes ejecutar lo siguiente:\n\npip install tensorflow==2.11.0\npip install h5py\npip install typing-extensions\npip install wheel\n\ny al final:\n\npip install tensorflow-text~=2.11.0\n\ninstalar desde ya ipywidgets sino en algun momento saldra error, no recuerdo cuando dx\npip install ipywidgets\n\ninstalar esta version de protobuf porque habia error al momento de entrenar el modelo:\npip install protobuf~=3.20.0\n\n(hasta el momento esta entrenando en el primer epoch, pero esta mostrando estos alertas de error:)\n\n\nERROR:absl:module 'tensorflow_datasets.core' has no attribute 'utils'\nTraceback (most recent call last):\n  File \"/home/studio-lab-user/.conda/envs/gpt/lib/python3.9/site-packages/tensorflow_datasets/__init__.py\", line 59, in <module>\n    from tensorflow_datasets import audio\n  File \"/home/studio-lab-user/.conda/envs/gpt/lib/python3.9/site-packages/tensorflow_datasets/audio/__init__.py\", line 19, in <module>\n    from tensorflow_datasets.audio.commonvoice import CommonVoice\n  File \"/home/studio-lab-user/.conda/envs/gpt/lib/python3.9/site-packages/tensorflow_datasets/audio/commonvoice.py\", line 27, in <module>\n    import tensorflow_datasets.public_api as tfds\n  File \"/home/studio-lab-user/.conda/envs/gpt/lib/python3.9/site-packages/tensorflow_datasets/public_api.py\", line 57, in <module>\n    deprecated = core.utils.docs.deprecated(deprecated)\nAttributeError: module 'tensorflow_datasets.core' has no attribute 'utils'\nEpoch 1/10\n2023-07-19 22:04:00.467755: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:57] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. gpt2_causal_lm/gpt2_backbone/embeddings_dropout/dropout/random_uniform/RandomUniform\n2023-07-19 22:04:01.556077: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n 682/2425 [=======>......................] - ETA: 13:11 - loss: 3.4543 - sparse_categorical_accuracy: 0.3821","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"HVTm8HvfnoEj"}},{"cell_type":"markdown","source":"## Introducción a los Modelos de Lenguaje Generativos de Gran Escala (LLMs)\n\nLos Modelos de Lenguaje Grande (LLMs, por sus siglas en inglés) son un tipo de modelos de aprendizaje automático que se entrenan en un gran corpus de datos de texto para generar resultados en varias tareas de Procesamiento del Lenguaje Natural (NLP, por sus siglas en inglés), como generación de texto, respuesta a preguntas o traducción automática.\n\nLos LLMs generativos se basan típicamente en redes neuronales de aprendizaje profundo, como la arquitectura [Transformer](https://arxiv.org/abs/1706.03762) inventada por investigadores de Google en 2017, y se entrenan con grandes cantidades de datos de texto, a menudo involucrando miles de millones de palabras. Estos modelos, como [LaMDA](https://blog.google/technology/ai/lamda/) y [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) de Google, se entrenan con conjuntos de datos extensos provenientes de diversas fuentes de datos, lo que les permite generar resultados para múltiples tareas. El núcleo de los LLMs generativos consiste en predecir la siguiente palabra en una oración, a menudo conocido como **Preentrenamiento LM Causal**. De esta manera, los LLMs pueden generar texto coherente basado en las indicaciones del usuario. Para obtener una discusión más pedagógica sobre los modelos de lenguaje, puedes consultar la [clase de Stanford CS324 LLM](https://stanford-cs324.github.io/winter2022/lectures/introduction/).","metadata":{"id":"Jz1PYPIczb74"}},{"cell_type":"markdown","source":"## Instalamos KerasNLP e Importamos Dependencias","metadata":{}},{"cell_type":"code","source":"!pip install -q keras-nlp","metadata":{"id":"mnDHnN1Yzb72","outputId":"498b8689-12f3-452b-ef66-83150f9363d8","tags":[],"execution":{"iopub.status.busy":"2023-07-24T22:10:30.483930Z","iopub.execute_input":"2023-07-24T22:10:30.484625Z","iopub.status.idle":"2023-07-24T22:10:46.163452Z","shell.execute_reply.started":"2023-07-24T22:10:30.484578Z","shell.execute_reply":"2023-07-24T22:10:46.162126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras_nlp\nimport tensorflow as tf\nfrom tensorflow import keras\nimport time\nimport sys\n\nprint(\"Versión de keras-nlp:\", keras_nlp.__version__)\nprint(\"\\nVersión de TensorFlow:\", tf.__version__)\nprint(\"\\nVersión de Python:\", sys.version)\n\nprint(\"\\nVersión de Cudnn:\")\n!cat /usr/include/x86_64-linux-gnu/cudnn_v*.h | grep CUDNN_MAJOR -A 2\n!cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2\n\n\nprint(\"\\nVersión de CUDA :\")\n!nvcc --version\nprint(\"\\nDispositivo GPU :\", tf.config.list_physical_devices('GPU')[0].name)\nprint(\"\\nEscpecificaciones de gpu:\")\n!nvidia-smi","metadata":{"id":"yWKcA4SHzb73","tags":[],"outputId":"d94a4d66-b9c9-4570-fc6b-7870cffd5a8c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introducción a KerasNLP\n\nLa construcción y entrenamiento de Modelos de Lenguaje Grande (LLMs) desde cero son complejos y costosos. Afortunadamente, existen LLMs pre-entrenados disponibles para su uso inmediato. [KerasNLP](https://keras.io/keras_nlp/) proporciona una amplia variedad de puntos de control pre-entrenados que te permiten experimentar con modelos de última generación sin necesidad de entrenarlos desde cero.\n\nKerasNLP es una biblioteca de procesamiento del lenguaje natural que brinda soporte a los usuarios a lo largo de todo su ciclo de desarrollo. KerasNLP ofrece tanto modelos pre-entrenados como componentes modulares, lo que permite a los desarrolladores reutilizar fácilmente los modelos pre-entrenados o construir sus propios LLM.\n\nEn pocas palabras, para los LLMs generativos, KerasNLP ofrece:\n\n- Modelos pre-entrenados con el método `generate()`, por ejemplo en, `keras_nlp.models.GPT2CausalLM` y `keras_nlp.models.OPTCausalLM`.\n- Clase de muestreo (Sampler) que implementa algoritmos de generación como Top-K, Beam y búsqueda contrastiva. Estos samplers se pueden utilizar para generar texto con modelos personalizados.\n\n","metadata":{"id":"wh2N8PNzzb74"}},{"cell_type":"markdown","source":"## Cargamos un modelo pre-entrenado de GPT-2 y generamos texto\n\nKerasNLP proporciona varios modelos pre-entrenados, como [Google\nBert](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\ny [GPT-2](https://openai.com/research/better-language-models). Puedes ver\nla lista de modelos disponibles en el [repositorio de KerasNLP](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/models).\n\nEs muy fácil cargar el modelo GPT-2, como puedes ver a continuación:","metadata":{"id":"YieMpyQEzb75"}},{"cell_type":"code","source":"# Para acelerar el entrenamiento y la generación, utilizamos un preprocesador\n# de longitud 128 en lugar de la longitud completa de 1024.\n\npreprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n    \"gpt2_base_en\",\n    sequence_length=128,\n)\ngpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n    \"gpt2_base_en\", preprocessor=preprocessor\n)","metadata":{"id":"_5HgQlt-zb75","tags":[],"outputId":"03b0e6a9-0e49-46bd-a96e-9da21640a52c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez que el modelo está cargado, puedes usarlo para generar texto de inmediato. Ejecuta las celdas a continuación para probarlo. Es tan simple como llamar a la función generate() una sola vez.","metadata":{"id":"ki5OKmgRzb76"}},{"cell_type":"code","source":"start = time.time()\n\noutput = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\nprint(\"\\nGPT-2 output:\")\nprint(output)\n\nend = time.time()\nprint(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")","metadata":{"id":"Khn7wneEzb76","tags":[],"outputId":"ba160caa-60b3-4e51-f5ff-26f748c7f681"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Intenta con otro ejemplo:","metadata":{"id":"hUhMT575zb76"}},{"cell_type":"code","source":"start = time.time()\n\noutput = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\nprint(\"\\nGPT-2 output:\")\nprint(output)\n\nend = time.time()\nprint(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")","metadata":{"id":"AlORlhDwzb76","outputId":"3f18305b-c7f1-466c-960b-c408839264ef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observa cuán más rápido es el segundo llamado. Esto se debe a que el grafo computacional se compila con [XLA](https://www.tensorflow.org/xla) en la primera ejecución y se reutiliza en la segunda, en segundo plano.\n\nLa calidad del texto generado parece estar bien, pero podemos mejorarlo mediante el ajuste fino / afinación (fine-tuning.).","metadata":{"id":"GDdFVHsCzb76"}},{"cell_type":"markdown","source":"## Más sobre el modelo GPT-2 de KerasNLP\n\nA continuación, vamos a ajustar finamente el modelo para actualizar sus parámetros, pero antes de hacerlo, echemos un vistazo al conjunto completo de herramientas que tenemos para trabajar con GPT2.\n\nEl código de GPT2 se puede encontrar [aquí](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/). Conceptualmente, `GPT2CausalLM` se puede descomponer jerárquicamente en varios módulos en KerasNLP, todos los cuales tienen una función *from_preset()* que carga un modelo pre-entrenado:\n\n- `keras_nlp.models.GPT2Tokenizer`: El tokenizador utilizado por el modelo GPT2, que es un [codificador de pares de bytes](https://huggingface.co/course/chapter6/5?fw=pt).\n- `keras_nlp.models.GPT2CausalLMPreprocessor`: el preprocesador utilizado en el entrenamiento causal de GPT2. Realiza la tokenización junto con otros trabajos de preprocesamiento, como la creación de la etiqueta y la adición del token de finalización.\n- `keras_nlp.models.GPT2Backbone`: el modelo GPT2, que es una pila de `keras_nlp.layers.TransformerDecoder`. A esto generalmente se le denomina simplemente `GPT2`.\n- `keras_nlp.models.GPT2CausalLM`: envuelve a `GPT2Backbone`, multiplica la salida de `GPT2Backbone` por la matriz de embedding para generar logits sobre los tokens del vocabulario.","metadata":{"id":"DMUG5tCvzb76"}},{"cell_type":"markdown","source":"## Ajuste fino (Fine-tuning) en el conjunto de datos WikiHow\n\nAhora que tienes conocimientos sobre el modelo GPT-2 de KerasNLP, puedes dar un paso más para ajustar finamente el modelo y lograr que genere texto en un estilo específico, ya sea corto o largo, formal o casual. En este tutorial, utilizaremos el conjunto de datos de Wikihow como ejemplo.\n\n\n\n## Fine-tuning con el Dataset en Español\n\nTambién podemos ajustar finamente GPT2 en conjuntos de datos que no sean su idioma principal de entrenamiento. Esta parte muestra cómo ajustar finamente GPT2 en un conjunto de datos de wikihow en español para enseñar a nuestro modelo a convertirse en un sabelotodo.\n\nDebido a que GPT2 utiliza un codificador de pares de bytes (byte-pair encoder) y el conjunto de datos de pre-entrenamiento original contiene algunos caracteres de gran parte de idiomas, podemos utilizar el vocabulario original para ajustar finamente en un conjunto de datos en español.","metadata":{"id":"mKGfUONezb77"}},{"cell_type":"code","source":"!# Descargamos el dataset desde HuggingFace\n!git clone https://huggingface.co/datasets/daqc/wikihow-spanish","metadata":{"id":"Ct1gnrN5zb77","outputId":"9c0e410d-83ed-4bcf-ca98-37f0cee7a1d3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Echemos un vistazo dentro de los datos de muestra del conjunto de datos:\n","metadata":{"id":"RhQKRjNBzb77"}},{"cell_type":"code","source":"import json\n\n# Leer el archivo JSON\nwith open('wikihow-spanish/spanish.json', 'r') as file:\n    data = json.load(file)\n\n# Obtener la primera clave y el primer elemento del diccionario\nprimer_clave = next(iter(data))\nprimer_elemento = data[primer_clave]\n\n# Imprimir la clave y la estructura del primer elemento\nprint(\"Clave:\", primer_clave)\nprint(\"Estructura:\")\nprint(json.dumps(primer_elemento, indent=4))","metadata":{"id":"t1ATVaVJzb77","outputId":"3f509407-b00a-4f9e-d421-c99184cfb413"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En nuestro caso, al realizar la predicción de la siguiente palabra en un modelo de lenguaje, necesitamos obtener solamente la informacion de relevancia del dataset mediante el siguiente codigo:","metadata":{"id":"NDhxryFVzb77"}},{"cell_type":"code","source":"import os\nimport json\nfrom urllib.parse import urlsplit, unquote\n\nwikihow_data = []\nfor file in os.listdir(\"wikihow-spanish/\"):\n    if \".json\" not in file:\n        continue\n    full_filename = os.path.join(\"wikihow-spanish/\", file)\n    with open(full_filename, \"r\") as f:\n        content = json.load(f)\n        wikihow_data.append(content)\n\nparagraphs = []\nfor parent_url, sections in data.items():\n    decoded_parent_url = \"¿Cómo \" + unquote(urlsplit(parent_url).path.split(\"/\")[-1]).replace('-', ' ') + \"?\"\n    paragraph = f\"{decoded_parent_url}\\n\"\n    for section in sections:\n        section_data = sections[section]\n        title = section + \":\"\n        summary = section_data.get(\"summary\", \"\")\n        document = section_data.get(\"document\", \"\")\n        paragraph += f\"{title}\\n{summary}\\n{document}\\n\"\n    paragraphs.append(paragraph)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Damos un vistazo a los datos de ejemplo.","metadata":{"id":"KbIH0qeUaRIg"}},{"cell_type":"code","source":"print(\"Parrafo 1:\")\nprint(paragraphs[0])\n#print(\"Parrafo 2:\")\n#print(paragraphs[1])","metadata":{"id":"tsUmonjoaQwb","outputId":"c1d1c76d-d305-414d-ca38-354e6ada3802"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahora puedes realizar el proceso de fine-tuning del modelo usando la función *fit()* que ya conoces. Ten en cuenta que el `preprocessor` se llamará automáticamente dentro del método `fit` ya que `GPT2CausalLM` es una instancia de `keras_nlp.models.Task`.\n\nEste paso requiere bastante memoria de la GPU y mucho tiempo si quisiéramos entrenarlo hasta alcanzar un estado completamente entrenado. En este caso, solo utilizamos una parte del conjunto de datos con fines de demostración.","metadata":{"id":"9wkh--Eazb77"}},{"cell_type":"code","source":"import tensorflow_datasets as tfds\nimport tensorflow.keras as keras\nimport os\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Preparar el conjunto de datos\ntrain_ds = (\n    tf.data.Dataset.from_tensor_slices(paragraphs)\n    .batch(16)\n    .cache()\n    .prefetch(tf.data.AUTOTUNE)\n)\n\n# Definir el número de épocas a 1 para demostración\nnum_epochs = 10\n# Recorrer todo el conjunto de datos lleva mucho tiempo, solo tomemos `500`\n# train_ds = train_ds.take(500)\n\nlearning_rate = keras.optimizers.schedules.PolynomialDecay(\n    5e-4,\n    decay_steps=train_ds.cardinality() * num_epochs,\n    end_learning_rate=0.0,\n)\n\nloss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\ngpt2_lm.compile(\n    optimizer=keras.optimizers.Adam(learning_rate),\n    loss=loss,\n    weighted_metrics=[\"accuracy\"],\n)\n\n# Directorio para guardar los checkpoints\ncheckpoint_dir = \"checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Callback para guardar checkpoints por epoch\ncheckpoint_callback = ModelCheckpoint(\n    filepath=os.path.join(checkpoint_dir, \"model_{epoch:02d}.ckpt\"),\n    save_freq=\"epoch\",\n    save_weights_only=True,\n    save_best_only=False,\n    verbose=1\n)\n\n# Entrenar el modelo\nhistory = gpt2_lm.fit(train_ds, epochs=num_epochs, callbacks=[checkpoint_callback])\n","metadata":{"id":"me-ggq_Tzb77","outputId":"3e93cb1c-bab1-4577-95b3-bb2e51244239"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n##  Métricas y parámetros para evaluar el rendimiento y la eficiencia durante el entrenamiento.\n\nUna vez terminado el entrenamiento podemos visualizar y monitorear el rendimiento del modelo durante el entrenamiento, proporcionando métricas como pérdida, precisión, tasa de aprendizaje y eficiencia computacional.","metadata":{"id":"2qY8BXrtv8rG"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Obtener las métricas de pérdida, precisión y tasa de aprendizaje durante el entrenamiento\nloss = history.history['loss']\naccuracy = history.history['accuracy'] # Reemplaza 'accuracy' con 'weighted_accuracy' si estás utilizando esta métrica\nlearning_rate = np.array([learning_rate[step] for step in range(len(history.epoch))])\n\n# Obtener la eficiencia computacional\ntime_per_epoch = history.epoch[-1] / len(history.epoch)  # Tiempo por época\nefficiency = [time_per_epoch * (i + 1) for i in range(num_epochs)]\n\n# Graficar la pérdida, precisión, tasa de aprendizaje y eficiencia\nepochs = range(1, num_epochs + 1)\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 4, 1)\nplt.plot(epochs, loss, 'b', label='Pérdida')\nplt.title('Pérdida durante el entrenamiento')\nplt.xlabel('Épocas')\nplt.ylabel('Valor')\nplt.legend()\n\nplt.subplot(1, 4, 2)\nplt.plot(epochs, accuracy, 'r', label='Precisión')\nplt.title('Precisión durante el entrenamiento')\nplt.xlabel('Épocas')\nplt.ylabel('Valor')\nplt.legend()\n\nplt.subplot(1, 4, 3)\nplt.plot(epochs, learning_rate, 'g', label='Tasa de Aprendizaje')\nplt.title('Tasa de Aprendizaje durante el entrenamiento')\nplt.xlabel('Épocas')\nplt.ylabel('Valor')\nplt.legend()\n\nplt.subplot(1, 4, 4)\nplt.plot(epochs, efficiency, 'm', label='Eficiencia')\nplt.title('Eficiencia Computacional durante el entrenamiento')\nplt.xlabel('Épocas')\nplt.ylabel('Tiempo (segundos)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"C50bH5ynv6He","outputId":"39818c80-cb7f-4259-b014-c39085809e48"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez finalizado el ajuste fino, puedes generar texto nuevamente utilizando la misma función generate(). Esta vez, el texto estará más cerca del estilo de wikihow y la longitud generada será similar a la longitud preestablecida en el conjunto de entrenamiento.","metadata":{"id":"vWNubPGvzb78"}},{"cell_type":"code","source":"start = time.time()\n\n\noutput = gpt2_lm.generate(\"Rendimiento anualizado de una cartera de inversione\", max_length=200)\nprint(\"\\nGPT-2 output:\")\nprint(output)\n\nend = time.time()\nprint(f\"TIEMPO TOTAL TRANSCURRIDO: {end - start:.2f}s\")","metadata":{"id":"0GsZi7DUzb78","outputId":"2962860e-ebf8-4ae1-e65d-909251dc6184"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Acerca del Método de Muestreo\n\nEn KerasNLP, ofrecemos varios métodos de muestreo, como la búsqueda contrastiva, el muestreo Top-K y el muestreo de haz (beam sampling). Por defecto, nuestro `GPT2CausalLM` utiliza el muestreo Top-K, pero puedes elegir tu propio método de muestreo.\n\nAl igual que con el optimizador y las funciones de activación, hay dos formas de especificar tu propio muestreador personalizado:\n\n- Utilizar un identificador de cadena, como \"greedy\", si deseas utilizar la configuración predeterminada de esta forma.\n- Pasar una instancia de `keras_nlp.samplers.Sampler`, si deseas utilizar una configuración personalizada de esta forma.","metadata":{"id":"eL9KgIgXzb78"}},{"cell_type":"code","source":"# Use a string identifier.\ngpt2_lm.compile(sampler=\"top_k\")\noutput = gpt2_lm.generate(\"rendimiento anualizado de una cartera de inversiones\", max_length=200)\nprint(\"\\nGPT-2 output:\")\nprint(output)\n\n# Use a `Sampler` instance. `GreedySampler` tends to repeat itself,\ngreedy_sampler = keras_nlp.samplers.GreedySampler()\ngpt2_lm.compile(sampler=greedy_sampler)\n\noutput = gpt2_lm.generate(\"Cepilla a tu burro regularmente\", max_length=200)\nprint(\"\\nGPT-2 output:\")\nprint(output)","metadata":{"id":"bWRmzJBmzb78","outputId":"7573bd91-93dd-4ad2-8acb-e6268868f020"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para obtener más detalles sobre la clase `Sampler` de KerasNLP, puedes revisar el código [aquí](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/samplers).","metadata":{"id":"QGvtOoQRzb78"}},{"cell_type":"markdown","source":"## Guarda tu Modelo\n","metadata":{"id":"jPmGoVk64AuE"}},{"cell_type":"markdown","source":"*** Guarda el modelo: Utiliza el método model.save() de Keras para guardar todo el modelo, incluyendo la configuración y los pesos, en un archivo. Especifica una ruta y un nombre de archivo para guardar el modelo","metadata":{"id":"d2j2nDXCb5eA"}},{"cell_type":"code","source":"import os\nsave_dir = \"saved_model_h5\"\nos.makedirs(save_dir, exist_ok=True)\nmodel.save(\"saved_model_h5/model.h5\")","metadata":{"id":"56ovHweLb4qk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Si estás trabajando principalmente con TensorFlow y planeas utilizar el modelo en otros proyectos o plataformas compatibles con SavedModel, entonces tf.saved_model.save() puede ser más conveniente.","metadata":{"id":"CcL8nILicdH9"}},{"cell_type":"code","source":"import os\nsave_dir2 = \"saved_model_tf\"\nos.makedirs(save_dir2, exist_ok=True)\ntf.saved_model.save(gpt2_lm, saved_model_dir)","metadata":{"id":"SUskIFtacmZQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Restaura tu modelo desde un checkpoint\n","metadata":{"id":"jPmGoVk64AuE"}},{"cell_type":"code","source":"preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n    \"gpt2_base_en\",\n    sequence_length=128,\n)\ngpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n    \"gpt2_base_en\", preprocessor=preprocessor\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\n# Crear el modelo con la misma arquitectura\npreprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n    \"gpt2_base_en\",\n    sequence_length=128,\n)\ngpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n    \"gpt2_base_en\", preprocessor=preprocessor\n)\n\n# Directorio donde se encuentran los pesos guardados\ncheckpoint_dir = \"checkpoints/\"\n\n# Verificar si hay checkpoints existentes para cargar los pesos desde el último\ncheckpoint_files = [file for file in os.listdir(checkpoint_dir) if file.endswith('.ckpt.index')]\nif checkpoint_files:\n    latest_checkpoint = max(checkpoint_files)\n    print(\"Ultimo checkpoint: \", latest_checkpoint)\n    gpt2_lm.load_weights(os.path.join(checkpoint_dir, latest_checkpoint[:-6]))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss,acc = gpt2_lm.evaluate(train_ds, verbose=2)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\n\noutput = gpt2_lm.generate(\"Como conseguir diamante en minecraft\", max_length=200)\nprint(\"\\nGPT-2 output:\")\nprint(output)\n\nend = time.time()\nprint(f\"TIEMPO TOTAL TRANSCURRIDO: {end - start:.2f}s\")","metadata":{},"execution_count":null,"outputs":[]}]}